1. mobilenet_v1 - https://arxiv.org/pdf/1704.04861.pdf
 mobilenet_v2 - https://arxiv.org/pdf/1801.04381.pdf
mobilenet_v3 - https://arxiv.org/abs/1905.02244v5
Посмотрел еще пару кратких учебных роликов.

Главное отличие mobilenet_v2 от v1 в том, что здесь стараются медленее увеличивать кол-во каналов, при этом, сохраняя ту же полезную информацию. Достигается
это тем, что мы изначально расширяем размерность по каналам, далее идет depthwise-convolution, а потом мы используем 1x1 свертку, но уже для понижения кол-ва каналом,
соответственно компануя информацию.

Отличия mobilenet_v3 от v2: использования Squeeze and Excitation layer в mobilenet_v2 block и использование hard-нелинейностей, для более быстрых вычислений.
Squeeze and Excitation layer по сути взвешивает каждый input канал. То есть оценивает, на сколько каждый канал нам важен.

2/3. Скрипт run.py принимает 3 параметра: pretrained, mode, path.
pretrained может быть равен "T" или "F", если запустить с "T", то веса будут выгружены из файла соответствующему моду (v3_small/v3_large).
Если запустить с "F", то инициализиурется v3_small/v3_large в зависимости от mode и будет обучать EPOCHS эпох. EPOCHS глобальная переменная, можно изменить в самом верху.
Если mode == "L", то запустится v3_large, иначе v3_small.
Path - путь к папке с фотографиями, run.py выводить predictions на экран, вне зависимости от предудыщих параметров (модель либо обучится и выдаст предсказания, либо загрузит веса и выдаст предсказания).

Обучить модель и выдать предсказания: python3 run.py F S ./images
Выгрузить веса и выдать предсказания: python3 run.py T S ./images

Предобученые модели обучались на CIFAR-10, 5/6 эпох. С теми же параметрами(lr, gamma, ...) что и в train(model).

4. Инициализровать веса как-нибудь по умному для более быстрой сходимости. Использование NAS и NetAdapt для меньших вычисллительных мощностей.
Добавить channel_multiplier,  этого не сделал, он всегда равен 1.0.
Можно поэксперементировать с dropout_rate на последнем fully-connected слое. Tuning параметров для ускорения и улучшения обучения.
Улучшить code style и параметризовать глобальные параметры. Многие улучшения которые были в статье я добавил, это: swish и sigmoid заменить на
hard_swish/sigmoid и в последнем блоке conv(kernel=1) поставить после avgpool, для увеличения скорости.

